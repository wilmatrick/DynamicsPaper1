\subsection{Data Likelihood} \label{sec:likelihood}

As data we consider here the positions and velocities of a population of stars within a given survey selection function $\text{sf}(\vect{x})$,
\begin{eqnarray*}
D  =\{ \vect{x}_i,\vect{v}_i \mid && \text{(star $i$ in given stellar population)}\nonumber\\
&\wedge& (\text{sf}(\vect{x_i}) > 0) \}.
\end{eqnarray*}

We fit a model potential and DF (here: the qDF) which are specified by a number of fixed and free parameters,
\begin{eqnarray*}
\pmodel \equiv \{ p_\text{DF} , p_\Phi \}.
\end{eqnarray*}
The orbit of the $i$-th star in a potential with $p_\Phi$ is labeled by the actions $\vect{J}_i := \vect{J}[\vect{x}_i,\vect{v}_i\mid p_{\Phi}]$ and the DF evaluated for the $i$-th star is then $\text{DF}(\vect{J}_i \mid \pmodel) := \text{DF}(\vect{J}[\vect{x}_i,\vect{v}_i\mid p_{\Phi}] \mid p_\text{DF})$.\\

The likelihood of the data given the model is, following BR13,
\begin{eqnarray}
&&\mathscr{L}(D \mid \pmodel) \nonumber\\
&&\equiv \prod_i^{N_*} p(\vect{x}_i,\vect{v}_i \mid \pmodel) \nonumber\\
&&= \prod_i^{N_*} \frac{\text{DF}(\vect{J}_i \mid \pmodel) \cdot \text{sf}(\vect{x}_i)}{\int \Diff 3 x \Diff 3 v \  \text{DF}(\vect{J} \mid \pmodel) \cdot \text{sf}(\vect{x})}\nonumber\\
&&\propto \prod_i^{N_*} \frac{\text{DF}(\vect{J}_i \mid \pmodel)}{\int \Diff 3 x \  \rho_\text{DF}(R,|z| \mid \pmodel) \cdot \text{sf}(\vect{x})}, \label{eq:prob}
%\label{eq:likelihood}
\end{eqnarray}
where $N_*$ is the number of stars in $D$, and in the last step we used Equation \ref{eq:tracerdensity}. $\prod_i\text{sf}(\vect{x}_i)$ is independent of \pmodel{}, so we treat it as unimportant proportionality factor. We find the best fitting \pmodel{} by maximizing the posterior probability distribution $\pdf{}(\pmodel \mid D)$, which is, according to Bayes' theorem, proportional to the likelihood $\mathscr{L}(D\mid \pmodel)$ times a prior $p(\pmodel)$. We assume flat priors in both $p_\Phi$ and
\begin{eqnarray}
p_\text{DF} := \{ \ln h_R, \ln \sigma_{R,0}, \ln \sigma_{z,0}, \ln h_{\sigma,R}, \ln h_{\sigma,z} \} \label{eq:p_DF}
\end{eqnarray}
(see Section \ref{sec:qDF}) throughout this work. Then \pdf{} and likelihood can be used interchangeably.\\

The normalisation in Equation \ref{eq:prob} is a measure for the total number of tracers inside the survey volume,
\begin{equation}
M_\text{tot} \equiv \int \Diff 3 x \  \rho_\text{DF}(R,|z| \mid \pmodel) \cdot \text{sf}(\vect{x}).\label{eq:normalisation}
\end{equation}
In the case of an axisymmetric galaxy model and $\text{sf}(\vect{x})=1$ within the observation volume (as in most tests in this work), the normalisation is essentially a two-dimensional integral in the $R$-$z$ plane over $\rho_{DF}$. We evaluate the integrals using Gauss-Legendre quadratures of order 40. The integral over the azimuthal direction can be solved analytically. 
\\It turns out that a sufficiently accurate evaluation of the likelihood is computationally expensive, even for only one set of model parameters. This expense is dominated by the number of action calculations required, which in turn depends on the number of stars in the sample ($N_*$ action calculations) and the numerical accuracy of the tracer density grid in Equation \ref{eq:tracerdensity} needed for the likelihood normalization in Equation \ref{eq:normalisation} ($N_x^2 \times N_v^3$ action calculations). The accuracy has to be chosen high enough, such that the resulting numerical error 
\begin{equation}
\delta_{M_\text{tot}} \equiv \frac{M_\text{tot,approx}(N_x,N_v,n_\sigma) -  M_\text{tot} }{M_\text{tot}}\label{eq:relerrlikelihood}
\end{equation}
\Wilma{[TO DO: make sure every Mtottrue is replaced by Mtot]}
does not dominate the log-likelihood, i.e.,
\begin{eqnarray}
& &\log \mathscr{L}_\text{approx}(\pmodel \mid D) \nonumber\\
&& = \sum_i^{N_*} \log \text{DF}(\vect{J_i} \mid \pmodel) - N_* \log(M_\text{tot})\nonumber\\
&& - N_* \log (1 + \delta_{M_\text{tot}}),\label{eq:loglikelihood_relerr}
\end{eqnarray}
with
\begin{eqnarray}
N_* \log (1 + \delta_{M_{tot}}) \lesssim 1.\nonumber
\end{eqnarray}
Otherwise numerical inaccuracies could lead to systematic biases in the potential and DF recovery. For data sets as large as $N_* = 20,000$ stars, which in the age of Gaia could very well be the case \HW{[TO DO: Really???]}, one needs a numerical accuracy of 0.005\% in the normalisation. Figure \ref{fig:norm_accuracy} demonstrates that the numerical accuracy we use in the analysis, $N_x=16$, $N_v=24$ and $n_\sigma=5$, does satisfy this requirement. This is slightly higher than in BR13, where $N_* \sim 100$ \Wilma{[TO DO: CHECK]}.\\

%====================================================================

%FIGURE: accuracy in the likelihood normalisation 

\begin{figure*}
\centering
\plotone{figs/normalisation_accuracy_4.eps}
\caption{Relative error  $\delta M_\text{tot}$ of the likelihood normalization in Equation \ref{eq:relerrlikelihood} depending on the accuracy of the grid-based density calculation in Equation \ref{eq:tracerdensity} (and surrounding text) in five spherical observation volumes with different radius $r_\text{max}$. (Test \ref{test:norm_accuracy} in Table \ref{tbl:tests} summarizes the model parameters.) The tracer density in Equation \ref{eq:tracerdensity} is calculated on $N_x\times N_x$ spatial grid points in $R \in [R_\odot \pm r_\text{max}]$ and $|z| \in [0,r_\text{max}]$. The integration over the velocities is performed with Gauss-Legendre quadratures of order $N_v$ within an integration range of $\pm n_\sigma$ times the dispersion $\sigma_R(R)$ and $\sigma_z(R)$ (and $[0,1.5v_\text{circ}]$ in $v_T$). (We vary $N_x$, $N_v$ and $n_\sigma$ separately and keep the other two fixed at the values indicated above the columns.) We calculate the ``true'' normalization in Equation \ref{eq:relerrlikelihood} with high accuracy as $M_\text{tot} \approx M_\text{tot,approx}(N_x=20,N_v=56,n_\sigma=7)$. The black dots indicate the accuracy used in our analyses: It is better than $0.001\%$ (dotted line). We find that the accuracy depends on the \emph{spatial} resolution of the grid and requires more accurate integrations over the \emph{velocity} for larger volumes within which the density varies more strongly.}
\label{fig:norm_accuracy}
\end{figure*}


%====================================================================

Measurement uncertainties of the data have to be incorporated in the likelihood. We assume Gaussian uncertainties in the observable space $\vect{y} \equiv (\tilde{\vect{x}},\tilde{\vect{v}})=(\text{RA},\text{DEC},(m-M),\mu_\text{RA},\mu_\text{DEC},v_\text{los})$, i.e. the $i$-th star's observed $\vect{y}_i$ are drawn from the normal distribution $N[{\vect{y}'}_i,\delta \vect{y}_i]$, with ${\vect{y}_i}'$ being the star's true phase-space position and $\delta \vect{y}_i$ its uncertainty . Stars follow the distribution function (DF$(\vect{y}') \equiv$ DF$(\vect{J}[\vect{y}' \mid p_\Phi] \mid p_\text{DF})$ for short), convolved with the measurement uncertainties $N[0,\delta \vect{y}]$ \Wilma{[TO DO: CHECK AGAIN]}. The selection function sf$(\vect{y})$ acts on the space of (error affected) observables. Then the probability of one star becomes
\begin{eqnarray*}
&&\tilde{p}(\vect{y}_i \mid p_\Phi,p_\text{DF},\delta \vect{y}_i)\\
& \equiv& \frac{\text{sf}(\vect{y}_i) \cdot \int \Diff{6} y' \  \text{DF}(\vect{y}') \cdot N[\vect{y}_i,\delta \vect{y}_i]}{\int \Diff{6}y'  \  \text{DF}(\vect{y}')  \cdot  \int \Diff{6} y \  \text{sf}(\vect{y})  \cdot N[\vect{y}',\delta \vect{y}_i]}.
\end{eqnarray*}
In the case of uncertainties in distance or (RA,DEC), the evaluation of this is computational expensive - especially if the stars have heteroscedastic $\delta \vect{y}_i$. In practice we apply the following approximation,
\begin{eqnarray}
&&\tilde{p}(\vect{y}_i \mid p_\Phi,p_\text{DF},\delta \vect{y}_i) \nonumber\\
&&\approx \frac{ \text{sf}(\vect{x}_i)}{M_\text{tot}} \cdot \frac{1}{N_\text{error}} \sum_n^{N_\text{error}}  \text{DF}(\vect{x}_i,\vect{v}[\vect{y}'_{i,n}]) \label{eq:errorconv}
\end{eqnarray}
with
\begin{eqnarray}
\vect{y}'_{i,n} \sim N[\vect{y}_i,\delta \vect{y}_i]\nonumber
\end{eqnarray}
We calculate the convolution using Monte Carlo (MC) integration with $N_\text{error}$ samples. The above approximation assumes that the star's \emph{position} $\vect{x}_i$ is perfectly measured. As the selection function is also velocity independent, this simplifies the normalisation drastically to Equation \ref{eq:normalisation}. Measurement uncertainties in $\mathrm{RA}$ and $\mathrm{DEC}$ are often negligible anyway. The uncertainties in the Galactocentric \emph{velocities} $\vect{v}_i = (v_{R,i},v_{T,i},v_{z,i})$ depend besides on $\delta \vect{\mu}$ and $\delta v_\text{los}$ also on the distance and its uncertainty, which we do \emph{not} neglect when drawing MC samples $\vect{y}'_{i,n}$ from the full uncertainty distribution $N[\vect{y}_i,\delta \vect{y}_i]$. Figure \ref{fig:isoSphFlexErrConv_MC_vs_error} demonstrates that in the absence of position uncertainties the $N_\text{error}$ needed for the convolution integral to converge depends as
\begin{equation*}
N_\text{error} \propto \delta v^2
\end{equation*}
on the uncertainties in the (1D) velocities.
\\A similar but only one-dimensional treatment of measurement uncertainties in $v_z$ was already applied by BR13.

%=============================================================

\begin{figure}
\centering
\subfigure[$N_* = 10,000$]{\includegraphics[width=60mm]{figs/isoSphFlexErrConv_MC_vs_error_2.eps}}
\subfigure[$N_* = 5,000$]{\includegraphics[width=60mm]{figs/Coming-Soon-Placeholder.eps}}
\caption{Number of Monte Carlo (MC) samples $N_\text{error}$ needed for the numerical convolution of the model probability with the measurement uncertainties in Equation \ref{eq:errorconv}, given the maximum velocity error $\delta v_\text{max}$ within the stellar sample. Unsufficent sampling introduces systematic biases in the parameter recovery as indicated in the legend. The relation found here, $N_\text{error} \propto \delta v_\text{max}^2$, was distilled from a set of analyses of mock data sets with different proper motion uncertainties $\delta \mu \in [2,5]~\text{mas yr}^{-1}$ in the absence of distance errors (see Test \ref{test:isoSphFlexErrConv_MC_vs_error} in Table \ref{tbl:tests}). The proper motion error $\delta \mu$ translates to heteroscedastic \Wilma{[TO DO: make sure that this word is written correctly everywhere.]} velocity errors according to $\delta v [\text{km s}^{-1}] \equiv 4.74047 \cdot r[\text{kpc}] \cdot \delta \mu [\text{mas yr}^{-1}]$, with $r$ being the distance of the star from the Sun. Stars with larger $\delta v$ require more $N_\text{error}$ for the integral over its measurement uncertainties to converge. We therefore show how the $N_\text{error}$ needed for the potential \pdf{} of the whole data set to be converged, depends on the largest velocity error $\delta v_\text{max} \equiv \delta v(r_\text{max})$ within the data set. We used $N_\text{error} = 800$ and  $1200$ for $\delta \mu \leq 3 \text{mas yr}^{-1}$ and $\delta \mu > 3 \text{mas yr}^{-1}$, respectively, as the reference for the converged convolution integral (see also left panels in Figure \ref{fig:isoSphFlexErrConv_bias_vs_SE}). \Wilma{[TO DO: no units in legend] [TO DO: some of the 25 MC sample analyses have to be re-done.] [TO DO: Replace lower plot with new plot with $N_* = 5,000$] [TO DO: Use $N_*$ everywhere where applicable, no $N_\text{sample}$] [TO DO: Introduce $N_*$ somewhere.] } \Wilma{[TO DO: Comment from Jo: I think it is important to test, if the MC vs error plot depends on number pf stars. Maybe test it with less stars (5000), to test this quickly. Naively, I would expect a large depedence on Ndata.]}}
\label{fig:isoSphFlexErrConv_MC_vs_error}
\end{figure}

%=============================================================