%% Document class
\documentclass[12pt,preprint]{aastex}

%general packages
\usepackage{amsmath}	%for \text{} in math mode
\usepackage{ mathrsfs } %for likelihood L

%%Figure packages
\usepackage{grffile}	%for dots in filenames

%% Custom macros
\newcommand{\vect}[1]{\boldsymbol{#1}} % Uncomment for BOLD vectors.
%\newcommand{\vect}[1]{\vec{#1}} % Uncomment for ARROW vectors.
\newcommand*\diff{\mathop{}\!\mathrm{d}}
\newcommand*\Diff[1]{\mathop{}\!\mathrm{d^#1}}
 
%% Abbreviations
\shorttitle{Action-based Dynamical Models for the Milky Way}
\shortauthors{Trick et al.}

\begin{document}

%% Title
\title{Action-based Dynamical Models for the Milky Way Disk:\\
    How to deal with "Real World" Issues}

%% Authors    
\author{W. Trick\altaffilmark{1,2} and H.-W. Rix\altaffilmark{1}}
\affil{Max-Planck-Institute for Astronomy, Heidelberg}
\email{trick@mpia.de}
\and
\author{J. Bovy\altaffilmark{3,4}}
\affil{Institute for Advanced Study, Princeton, NJ}

%% Affiliations
\altaffiltext{1}{Max-Planck-Institut f\"ur Astronomie, K\"onigstuhl 17, D-69117 Heidelberg, Germany}
\altaffiltext{2}{Correspondence should be addressed to trick@mpia.de.}
\altaffiltext{3}{Institute for Advanced Study, Einstein Drive, Princeton, NJ 08540, USA}
\altaffiltext{4}{Hubble fellow}



%% Abstract
\begin{abstract}
Starting point for abstract: my old poster abstract. [TO DO] We aim to recover the Milky Way's gravitational potential using action-based dynamical modeling (cf. Bovy \& Rix 2013, Binney \& McMillan 2011, Binney 2012). This technique works by modeling the observed positions and velocities of disk stars with an equilibrium, three-integral quasi-isothermal distribution function. In preparation for the application to stellar phase-space data from Gaia, we create and analyze a large suite of mock data sets and we develop qualitative "rules of thumb" for which characteristics and limitations of data, model and code affect constraints on the potential most. We investigate sample size and measurement errors of the data set, size and shape of the observed volume, numerical accuracy of the code and action calculation, and deviations of the data from the assumed family of axisymmetric model potentials and distribution functions. This will answer the question: What kind of data gives the best and most reliable constraints on the Galaxy's potential?
\end{abstract}

%% Keywords
\keywords{Galaxy: disk --- Galaxy: fundamental parameters --- Galaxy: kinematics and dynamics --- Galaxy: structure}

\section{Introduction}

[TO DO]

\paragraph{Collection of thoughts for the introduction:} \textit{(Text is not yet perfect or concise, but should serve as a starting point to setup a basic structure for the introduction. The text will then have to be shortened, redundant formulations have to be removed, phrasing has to be improved and everything has to be supported with appropriate references.)}
\begin{itemize}
\item \textbf{Our modelling method in a nutshell:} We fit simultaneously a model for the Galaxy's gravitational potential and an orbit distribution function (df) to stellar phase-space data. To turn a star's position and velocity into a full orbit, we need the gravitational potential in which the star moves. We assume that we know a family of orbit distribution functions that are close enough to the real distribution of orbits. In this case the stellar orbits calculated within a proposed potential will only follow such a df, if this potential model is close enough to the true potential.
\\Or in other words: We need the potential to calculate orbits. At the same time, if we \textit{know} the true orbits, we can deduce the true potential from them. To find the true orbits, we make use of the predictive power of an orbit distribution function.

\item \textbf{Motivation to use this modelling technique in the Milky Way:} Bovy et al. 2012 .... [TO DO]

\item \textbf{Introducing orbits and actions:} There are different ways to describe stellar orbits. The most obvious is to give the stars position and velocity vector at each point in time, by evaluating the potential forces that act on the star in each time step. Most orbits in realistic galaxy potentials  are however not closed, so we would have to integrate the orbit forever. Another, much more convenient way to describe orbits, are so called integrals of motion. These integrals are functions of the star's time-dependent position and velocity, but are themselves constants in time, i.e. conserved quantities. The most obvious integral in static potentials is the energy of the orbit. Symmetries in potentials frequently allow more than one integral: In spherical potentials all three components of the angular momentum are conserved. In many axisymmetric potentials there is, in addition to the energy $E$ and vertical component of the angular momentum $L_z$, a third non-classical integral of motion $I_3$, which has however no easy physical meaning.  (Binney \& Tremaine, Galactic Dynamics)\\
Because any function of integrals is an integral of motion itself, it is possible to construct integrals that have both very convenient properties and intuitive physical meanings. One such a set are the so-called actions. In axisymmetric potentials they are frequently called the radial action $J_R$, the vertical action $J_z$ and the $\phi$-action, which is simply the vertical component of the angular momentum, $L_z$. The radial action and vertical action quantify the amount of oscillation in radial and vertical direction that the orbit exhibits.  Actions are constructed in such a way, that they are not only integrals, but also correspond to the momenta in a set of canonical coordinates. The canonical conjugate positions of the actions are the so-called angles, which have the convenient properties, that they increase strictly linearly in time while the star moves along the orbit. They are periodic in $2\pi$ and the frequencies by which they change are functions of the actions. In the action-angle coordinate system, the only thing we need to fully describe an orbit in an axisymmetric potential are therefore just three fixed numbers, the actions. 

\item \textbf{Using actions for distribution functions:} Actions are therefore the natural coordinates of orbits and each point in action space corresponds to one specific orbit in a given potential. It is often used in dynamical modelling, e.g. in the Schwarzschild superposition method (source???), to reconstruct a galaxy by superimposing different orbits and populating them with stars. In this way these kind of methods construct orbit distribution functions for galaxies, which are at the same time distribution functions in action space. Because angles increase linearly in time, when a star moves along its orbit, stars are uniformly distributed in angle space. Therefore a orbit distribution function in terms of actions and a uniform distribution of stars in angle-space can be directly mapped to a distribution of stars in canonical configuration phase-space, measurable stellar positions and velocities. While a stellar distribution in configuration space is six-dimensional, the distribution in action-angle space is effectively three-dimensional, because of the uniformity in angles. (Rewrite, too verbose...)

\item \textbf{Why should we care about actions in realistic galaxies?} In reality galaxies have rarely perfectly static and axisymmetric potentials, which drastically reduces the number of conserved quantites along orbits. In static non-axisymmetric  potentials there can still be two integrals of motion, angular momentum however is no longer conserved. The Milky Way's disk might have an overall axisymmetric appearance, but is perturbed by spiral arms. The strongest deviation from axisymmetry in the Galaxy is the bar, which also causes the Galactic potential to vary slowly in time. The stirrs up the stars of the disk and the potential and causes radial migration of the orbits (Reference???), orbits change and with them the actions. One could wonder if, under such non-axisymmetric, non-static potential conditions, the assumption and treatment of globally conserved actions in the Milky Way is still a sensible approach. First of all, actions are the natural way to treat orbits and they can be locally defined, even if they might not be globally conserved. As long as we care about orbits, we should care about actions. An orbit carries information about the star's past, about where the star was born and which tidal processes might have carried it away from its inital orbit. Together with the chemistry of the stars, which determined by their place of birth, their current orbits are valuable diagnostics for the evolution and structure of the Milky Way. Secondly, gravitational processes do only in the most extreme cases completely change the actions. In a slowly changing potential, where orbits adapt adiabatically to those changes, actions are conserved (Binney \& Tremaine, Galactic Dynamics). And even during bar-induced radial migration at least the vertical actions are conserved and will continue to carry some amount of information about the stars' inital orbit distribution.\\

[TO DO] (Maybe cite Potzen 2015, who showed that analysing aspherical systems in spherical actions can still be a powerful tool, when used with care...)

\item \textbf{Why should we care about an axisymmetric "best fit" model for the Milky Way disk?} One of the key assumptions of our modelling technique is the assumed axisymmetry of the Milky Way's gravitational potential, especially its disk. As we discussed already in the previous paragraph, this assumption is indeed only an approximation to the real disk, which has a much richer structure and more complicated potential, with spiral arms and ring-like structures (like the Monocerros ring), with a warp and a flare in the outer disk (references????). Also the Milky Way's halo has substructure, a multitude of streams (references???) and shell-like overdensities (reference???). The ultimate goal will be to find and identify substructures observationally and describe theoretically the structure and evolution of potential perturbations. Our method and efforts  to extract information about the axisymmetric Milky Way potential from disk stars aims to create a reliable and well-constraint basis for these endevours: The best possible axisymmetric approximation to the Milky Way's potential could serve as a realistic equilibrium model from which a description of non-axisymmetric tidal perturbations can be theoretically established by perturbation theory. It will also help a great deal to identify sub-structures, e.g. to find and orbitally connect tidal streams, which in return will then give better constraints on the deviations from axisymmetry. Many modelling and techniques, both purely gravitational, but also chemo-dynamical, can greatly profit from a good axisymmetric model for the galaxy: While we are still far away from knowing the MW's potential all over the place, an axisymmetric model will be the best reference to turn phase-space coordiantes into whole orbits. And orbits are the diagnostics that carry information from everywhere in the galaxy into the solar neighbourhood, where we can hope to exploit them. (Some overlap with section before. How to better structure these two sections and assign the arguments more clearly to "axisymmetric disk" or "actions"?) 

\item \textbf{Previous results with this modelling technique:} Bovy \& Rix 2013 ... [TO DO]
\begin{itemize}
\item disk scale length $R_d = 2.15 \pm 0.14 \text{ kpc }$ (Bovy \& Rix 2013)
\item disk is maximal (Bovy \& Rix 2013)
\item slope of dark matter halo $\alpha < 1.53$ (Bovy \& Rix 2013)
\end{itemize}

\item \textbf{What do we already know about the axisymmetric MW disk (from other references)?} [TO DO]
\begin{itemize}
\item rotation curve is well-known (reference???)
\end{itemize}

\item \textbf{What is there left to learn about the axisymmetric MW disk?} (as Jo asked at the Santa Barbara conference... [TO DO]
\begin{itemize}
\item separation of different MW component is still unclear: individual density profiles, contributions to total pot
\item thin/thick disk vs. continuum of exponential disks
\item dark matter at smaller radii
\item slope \& shape of dark matter halo (current state of knowledge?)
\end{itemize} 



\item \textbf{Motivating this method characterization in anticipation of GAIA:} [TO DO]
\end{itemize}

\section{Method}

\subsection{Actions}

[TO DO]


\subsection{Distribution function} \label{sec:qDF}

[TO DO]

\subsection{Potential models}  \label{sec:potentials}

[TO DO] Mention different ways to calculate actions in different potentials.

\subsection{Mock Data}

One goal of this work is to test how the loss of information in the process of measuring stellar phase-space coordinates can affect the outcome of the modelling. To investigate this, we assume first that our measured stars do indeed come from our assumed families of potentials and distribution functions and draw mock data from a given true distribution. In further steps we can manipulate and modify these mock data sets to mimick observational effects.\\
The distribution function is given in terms of actions and angles. The obvious procedure would be to draw $\vect{J}_i$ from qDF$(\vect{J}_i \mid p_{DF})$ and $\vect{\theta}_i$ between 0 and $2\pi$, transforming this to $(\text{x}_i,\text{v}_i)$ in the given potential and rejecting all stars that are outside the observed volume. The transformation $(\vect{J}_i,\vect{\theta}_i) \longrightarrow (\vect{x}_i,\vect{v}_i)$ is however difficult to perform and computationally much more expensive than the transformation $(\vect{x}_i,\vect{v}_i) \longrightarrow (\vect{J}_i,\vect{\theta}_i)$. The observed volume is also much smaller than the whole galaxy and the fraction of rejected stars would be enormous. We propose a fast and simple two-step method for drawing mock data from an action distribution function in a given observed volume.
[TO DO]


\subsubsection{Preparation: Tracer density} \label{sec:density}

One crucial point in creating the mock data, as well as in the analysis, is to calculate the spatial tracer density $\rho(\vect{x} \mid p_{\Phi},p_{DF})$ for a given distribution function with parameters $p_{DF}$ in a potential with parameters $p_{\Phi}$. We do this by integrating the axisymmetric distribution function over the velocity at $N_\text{spatial} \times N_\text{spatial}$ regular grid points in the $(R,z)$ plane, using a Gauss-Legendre quadrature of order $N_\text{velocity}$ in each of the three velocity components. The velocity distribution according to the qDF at a given $(R_i,z_i)$ looks approximately Gaussian for $v_R$ and $v_z$. The $v_T$ distribution peaks somewhere around $v_{circ}(R_\odot)$ and $v_T > 0$ (cf. \S ?????). We approximate the integration over the velocity therefore as
\begin{eqnarray}
\rho(R,|z| \mid p_{\Phi},p_{DF}) &=& \int_{-\infty}^{\infty} \text{qDF}(J[R,z,\vect{v} \mid p_{\Phi}] \mid p_{DF}) \Diff3\vect{v}\nonumber\\
&\approx& \int_{-N_\text{sigma} \sigma_R(R \mid p_{DF})}^{N_\text{sigma} \sigma_R(R \mid p_{DF})} \int_{-N_\text{sigma}\sigma_z(R \mid p_{DF})}^{N_\text{sigma} \sigma_z(R \mid p_{DF})} \int_{0}^{1.5 v_{circ}(R_\odot)}   \nonumber\\
& & \hspace{1cm} \text{qDF}(J[R,z,\vect{v} \mid p_{\Phi}] \mid p_{DF}) \diff v_T \diff v_z \diff v_R, \label{eq:tracerdensity}
\end{eqnarray}
where $\sigma_R(R \mid p_{DF})$ and $\sigma_z(R \mid p_{DF})$ are the star's radial and vertical velocity dispersion according to the qDF and given by eq. (???) and (???) and $N_\text{sigma}$ has to be large enough. The size of the $N_\text{spatial} \times N_\text{spatial}$ grid in $(R,z)$ is chosen cover the extent of the observed volume for $z>0$. We interpolate then over this grid to be able to evaluate the density at each $(R,z)$ within the observed volume. The total number of grid points and therefore actions that need to be calculated are $N_\text{spatial}^2 \cdot N_\text{velocity}^3$. Fig. ??? shows the importance of choosing $N_\text{spatial}$, $N_\text{velocity}$ and $N_\text{spatial}$ sufficiently large in order to get the density with an acceptable numerical accuracy. For the creation of the mock data we use $N_\text{spatial} = 20$, $N_\text{velocity} = 40$ and $N_\text{sigma}=5$.

\subsubsection{Step 1: Drawing positions from the observed volume}

To get $\vect{x}_i$ for our mock data stars, we first sample random positions $(R_i,z_i,\phi_i)$ uniformly from the observed volume. Making use of the geometric shapes of our simple observed volumes (spheres, cylinders, wedges...) we can do this efficiently with inverse transform Monte Carlo sampling. In a second step we then apply a rejection Monte Carlo method to these positions using the pre-calculated, interpolated density grid $\rho(R,|z| \mid p_{\Phi},p_{DF})$. In an optional third step, if we want to apply a non-uniform selection function, sf$(\vect{x}) \neq $ const. within the observed volume, we use the rejection method a second time on the set of positions we got from the last step. This procedure can be repeated until the required number of positions are reached. The sample then follows the required distribution
\begin{equation*}
\vect{x}_i \longrightarrow p(\vect{x}) \propto \rho(R,z \mid p_{\Phi},p_{DF}) \times \text{sf}(\vect{x}).
\end{equation*}

\subsubsection{Step 2: Drawing velocities according to the distribution function}

The velocities are independent of the selection function and observed volume. For each of the positions $(R_i,z_i)$ found in step 1 we now sample velocities directly from the qDF$(R_i,z_i,\vect{v} \mid p_{Phi},p_{DF})$ using a rejection method. To reduce the number of rejected velocities, we use a Gaussian in velocity space as an envelope function, from which we first randomly sample velocities and then apply the rejection method to shape the Gaussian velocity distribution towards the velocity distribution predicted by the qDF. The envelope Gaussian peaks at $(v_R=0,v_T=max(\text{qDF}(R_i,z_i,0,vT,0 \mid p_{Phi},p_{DF})),v_z=0)$ and has a standard deviation of $(2\sigma_R(R_i),2\sigma_R(R_i),2\sigma_z(R_i))$. We then pick one of the sampled velocities $\vect{v}_i$ at and for each $(R_i,z_i)$ and arrive at our desired mock data sample
\begin{equation*}
(\vect{x}_i,\vect{v}_i) \longrightarrow p(\vect{x},\vect{v}) \propto \text{qDF}(\vect{x},\vect{v} \mid p_{\Phi},p_{DF}) \times \text{sf}(\vect{x}).
\end{equation*} 

\paragraph{Collection of possible tests and plots}

\begin{itemize}
\item *Diagram 1*: schematic flow chart of how to sample mock data (could be helpful for people, who want to sample mock data in action space and didn't know how to start, like me)
\item *Plot 2:* 2 triangle plots with (jr, lz, jz) on the axes to show the distribution of stars in action space within mock data sets - for a large sphere and a small sphere. (I thought it was very instructive to see how the spatial selection function shapes the distribution of actions, it also helped me understand which orbits have which actions.)
\item *Plot 3:* distribution of mock data set in real space: z vs. R. and vz vs. R, maybe for a hot and cold population? (maybe a bit boring? Would be however illustrative, that the mock data sampled from the qdf is indeed similar to something we could observe. Also: could make a 4 sigma contour in the vz vs. R plot, to show, that the choice of integration limits is important but 4 sigma should be sufficient.)
\end{itemize}

\subsection{Analysis}

The idea behind our modeling approach is that the orbits of the stars belonging to one MAP [TO DO: explain MAP???], calculated from a phase-space observation for each star within a proposal potential,  will only follow a distribution function from the family of qDFs (cf. \S\ref{sec:qDF}) if this propsal potential is (close to) the true potential in which the stars move. This opens up the possibility to fit the qDF and the potential simultaneously to the stellar phase-space data of one MAP, using the orbits of the stars. 

\subsubsection{Likelihood}

We're fitting the potential and the qDF to the data
\begin{eqnarray*}
D_j  =\{ \vect{x}_i,\vect{v}_i \mid \text{star $i$ belonging to MAP $j$}\},
\end{eqnarray*}
where  $\vect{x}_i$ and $\vect{v}_i$ are the position and velocity of one star. Our modeling takes place the galactocentric rest-frame with cylindrical coordinates $\vect{x} = (R,\phi,z)$ and velocity components in the corresponding coordinate directions $\vect{v} = (v_R,\phi,z)$. If the phase-space data is given in observed coordinates, position $\tilde{\vect{x}}=(\alpha,\delta,m-M)$ in right ascension $\alpha$, declination $\delta$ and distance modulus $m-M$ and velocity $\tilde{\vect{v}} = (\mu_\alpha,\mu_\delta,v_\text{los})$ as proper motions $\vect{\mu}=(\mu_\alpha,\mu_\delta)$ [TO DO: cos somwhere???] and line-of-sight velocity $v_\text{los}$, the data $(\tilde{\vect{x}},\tilde{\vect{v}})$ has to be converted first into the galactocentric rest-frame coordinates $(\vect{x},\vect{v})$ using the sun's position and velocity (cf. \S ???).

 by finding the maximum likelihood,
\begin{equation*}
\mathscr{L}(M \mid D_j)
\end{equation*}

To be able to control the number of degrees of freedom in the potential fit, we have to assume a certain family of potential models, parametrized by the parameters $p_\Phi$ (cf. \S\ref{sec:potentials}). The orbit of the $i$-th star in a potential with $p_\Phi$ is labeled by the actions $\vect{J}_i := \vect{J}[\vect{x}_i,\vect{v}_i\mid p_{\Phi}]$.

[TO DO] Don't forget: How to choose the fitting ranges.

\subsection{Measurement Errors}

[TO DO]

\section{Results}

\subsection{Verification of the Method}

\paragraph{Collection of possible tests and plots}

\begin{itemize}
\item *Plot 1:* two panels: 
\begin{itemize}
\item a) convergence of the normalisation vs. ngl\_vel (GL order of integrating the qdf over the velocities to get the density), 
\item b) convergence of the normalisation vs. n\_dens (number of grid points in each (R,z) at which the density is explicitely calculated, before interpolating and integrating over the volume to get the normalisation). \\ 
 This might not be a very exciting plot, but when we later show plots, that demonstrate e.g. how robust the method is against incompleteness, people might think, that in this case the normalization is not so important and time could be saved in calculating it. We know, that it is important to get the normalisation right. Plus, it proves, that possible biases when using the St\"ackel approximation are not due to a wrong normalisation.
 \end{itemize}
\item *Test 1:* Isochrone potential, 2 different b, 2 different populations, 5 different SF (isoSph test suite) \\
*Plot 2:* scatter plot (offset / stddev) vs. (stddev / true value [\%]) for b and one qdf parameter --> and panel with normal distribution. This plot could show 3 things:
\begin{itemize}
\item Central limit theorem is satisified --> method works.
\item Bigger volumes give better constraints.
\item hot populations seem to give tighter constraints on the potential.
\end{itemize}
\item *Plot 3:* Would be cool to have a plot, that shows that for the St\"ackel potential we don't get biases, but that there are some for the analytic Miyamoto-Nagai + power-law halo \& interpolated MW potential and therefore this bias is probably due to incorrect action calculation.
\item *Plot 4:* stddev ~ 1/sqrt(N)
\end{itemize}


\subsection{Do shape and position of the observation volume matter?}

\paragraph{Collection of possible tests and plots}

 *Test 1:* Compare results of wedges of same volume, but different positions and orientations. 
 \begin{itemize}
    \item I guess, the ones that demonstrate, that phi-coverage is much less important than R and z coverage, is boring, right? And I already have a plot in 3.1 that shows, that larger volumes are better. 
    \item This test suite was made with the MW-like potential and there seem to be biases, that are different for different volumes. If we say, okay, we have to deal with whatever biases we get, I could still include those volumes with good R AND z coverage, because for them the biases seem to be smaller.
    \item I might add a few more volumes, e.g. one with large vertical coverage at different positions
    \item Do we explicitely want to test, if it matters, if the radial coverage is larger or smaller the disk scale length, and the vertical coverage is larger or smaller than the disk scale height?\\
*Plot 1:* 
    a) cross section of volumes in R and z
    b) offset / stddev vs. stddev / true value [\%], that demonstrates, that it doesn't matter much for the potential recovery, if we have more radial or vertical coverage, and the position within the galaxy. 
     \end{itemize}
     
\subsection{What if our assumptions on the (in-)completeness of the data set are incorrect?}

\paragraph{Collection of possible tests and plots}

\begin{itemize}
\item  *Test 1:* isochrone potential, b=0.9 kpc, two populations, completeness$(d) = 1 - \epsilon \cdot d/Rmax$, where Rmax is radius of spherical selection function. Marginalize over vT in analysis. \\
*Plot 1*: Violin plot: x-axis - $\epsilon$. y-axis: b-parameter and one of the qdf parameters.
\item  *Test 2:* isochrone potential, two populations, incompleteness function that depends only on z. \\
*Plot 2*: violin plot
\end{itemize}

\subsection{What if our assumed distribution function differs from the star's DF?}

\paragraph{Collection of possible tests and plots}

\begin{itemize}
\item *Test 1:* mix hot and cold populations, 5 free qdf parameters in analysis!, use code that estimates the best velocity integration ranges. h\_sigma\_r \& h\_sigma\_z are the same for both populations, sigma\_r and sigma\_z have the same ratio, but are 50\% different for the two populations. h\_R is also 50\% different. Vary the fraction of pollution. Idea behind this: What if the stellar distribution has a different shape, e.g. added "wings", or had a different tracer density decrease with R. Would be however great, if we could show how the mixture of qdf's quanlitatively changes the shape of the df. Any ideas? \\
*Plot 1:* Violin plot: x-axis - fraction of pollution. y-axis: b-parameter and one or two qdf parameters.
\item *Test 2:* same as Test 1, but this time vary the degree of difference and make it 50\% pollution. Idea behind this: What happens, if we have errors in the abundances and mix different MAPs? For this it would be could to compare how much the qdf parameters of neighbouring MAPs differ and how big the difference between MAPs can be, such that it still can reproduce the potential. \\
*Plot 2:* Violin plot: x-axis - difference in qdf parameters. y-axis: b-parameter and one or two qdf parameters.
\end{itemize}

\subsection{What if our assumed potential model differs from the real potential?}

\paragraph{Collection of possible tests and plots}

*Test 1:* Try to recover a Miyamoto-Nagai disk + power-law halo potential by fitting a 2-component St\"ackel potential. \\
*Plot 1:* 
\begin{itemize}
   \item (R,z)-plane: color coding: difference between true potential's F\_R and best fit potential F\_R
    \item (R,z)-plane: color coding: difference between true potential's F\_z and best fit potential F\_z \\
    Any idea how to account for the error bars on the best fit potential?
\end{itemize}

\subsection{Effect of measurement errors on recovery of potential?}

\paragraph{Collection of possible tests and plots}

\begin{itemize}
\item *Plot 1:* The plot I had on the poster, which shows the number of MC samples needed for given maximum error. However, we still haven't tested, if this plot depends on: 
    * hotness of stars
    * number of stars
\item *Plot 2:* Some plot that shows, that our approximation of ignoring distance errors works. Any ideas?
\item *Test 1:* One selection function, one population, vary the size of the proper motion error (don't forget to adapt the number of MC samples needed) \\
*Plot 3:* (width of pdf) vs. (maximum velocity error / temperature parameter)
\end{itemize}



\section{Conclusion}

[TO DO]

\section{Questions that haven't been covered so far:}

\begin{itemize}
\item What limits the overall code speed?
\item What happens, when the errors are not uniform?
\item What if errors in distance matter for selection?
\item Deviations from axisymmetry: Take numerical simulations.
\end{itemize}

%===============================================

%FIGURE: Triangle plot, shape of likelihood, multi-variate Gaussian

\begin{figure}
%\plottwo{~/BovyCode/analyzeFlexible/plots/isoSphFlex/isoSphFlex_short_cold_2kpc_1d_DF:5_POT:2_triangle_MCMC_bw.eps}{~/BovyCode/analyzeFlexible/plots/isoSphFlex/isoSphFlex_short_cold_2kpc_1d_DF:5_POT:2_triangle_MCMC_color.eps}
\plotone{~/BovyCode/analyzeFlexible/plots/isoSphFlex/isoSphFlex_short_cold_2kpc_1d_DF:5_POT:2_triangle_MCMC_color.eps}
\caption{The likelihood in eq. (???) in the parameter space $\{p_\Phi,\ln(p_{DF})\}$ for one example mock data set. This mock data set has 20,000 stars and was created in a isochrone potential with $p_\Phi = \{v_{circ},b \}=\{230 \text{ km/s },0.9\text{ kpc } \}$, observed within a spherical volume around the sun of radius $r = 2 \text{ kpc }$, and represents a rather hot stellar population with DF parameters $p_{DF} = \{ h_R, \sigma_R, \sigma_z,h_{\sigma_R},h_{\sigma_z}\} =\{2 \text{ kpc }, 55 \text{ km/s }, 66 \text{ km/s }, 8 \text{ kpc }, 7 \text{ kpc }\} $.  The true parameters are marked by dotted lines. The dark, medium and bright purple contours in the 2D distributions represent 1, 2 and 3 sigma confidence regions, respectively, and show weak or moderate covariances. The likelihood here was sampled using MCMC (with flat priors in $p_\Phi$ and  $\ln(p_{DF})$ to turn the likelihood into a full posterior distribution function). Because only 10,000 MCMC samples were used to create the histograms shown, the 2D distribution has noisy contours. The dashed lines in the 1D distributions are Gaussian fits to the histogram of MCMC samples. This demonstrates very well that for such a large number of stars, the likelihood approaches the shape of a multi-variate Gaussian, as expected from the central limit theorem.}
\end{figure}

%FIGURE: width of likelihood propto 1/sqrt(N)

\begin{figure}
\plotone{~/BovyCode/analyzeFlexible/plots/sqrtNiso/sqrtNiso_Stddev_Vs_N.eps}
\caption{The width of the likelihood for two fit parameters found from analyses of 132 mock data sets vs. the number of stars in each data set. All mock data sets were created in an isochrone potential with $p_\Phi = \{v_{circ},b \}=\{230 \text{ km/s },0.9\text{ kpc } \}$, for a qDF with $p_{DF} = \{ h_R, \sigma_R, \sigma_z,h_{\sigma_R},h_{\sigma_z}\} =\{2 \text{ kpc }, 55 \text{ km/s }, 66 \text{ km/s }, 8 \text{ kpc }, 7 \text{ kpc }\} $ and within a spherical observation volume around the sun of radius $r = 3 \text{ kpc }$. The data sets have different sample sizes and contain between 100 and 40,000 stars, as indicated on the $x$-axis. The likelihood was evaluated on a grid in the parameters $\{b,\ln(h_R/8\text{kpc}),\ln(\sigma_{R}/230 \text{km/s}),\ln(h_{\sigma_R}/8\text{kpc}) \}$, while all other parameters were assumed to be known and kept at their true values. For each fit parameter the likelihood was then marginalized by summing over the grid and then a Gauss curve was fitted to the marginalized likelihood. The standard deviation of these best fit Gaussians $\Delta$ is shown on the $y$-axis for $b$ in kpc (red dots) and for $\ln(h_R/8\text{kpc})$ in dimensionless units (blue). The black lines are fits of the functional form $\Delta(N) \propto 1/\sqrt{N}$ to the data points  of both shown parameters. As can be seen, for large data samples the width of the likelihood behaves as expected and scales with $1/\sqrt{N}$ as predicted by the central limit theorem.} 
\end{figure}

%FIGURE: accuracy in the likelihood normalisation 

\begin{figure}
\plotone{~/BovyCode/analyzeFlexible/plots/normalisation_accuracy/normalisation_accuracy_2.eps}
\caption{Relative error of the likelihood normalization in eq. (???) depending on the accuracy of the density calculation in \S\ref{sec:density}. The different colors represent calculations for different radii of the spherical observation volume around the sun, as indicated in the legend. $N_\text{spatial}$ is the number of regular grid points in each $R$ and $z > 0$ within the observed volume on which the tracer density is evaluated according to eq. (\ref{eq:tracerdensity}). At each $(R,z)$ a Gauss-Legendre integration of order $N_{velocity}$ is performed over an integration range of $\pm N_\text{spatial}$ times the dispersion in $v_R$ and $v_z$ and $[0,1.5v_{circ}(R_\odot)]$ in $v_T$. To integrate the interpolated density over the observed volume to arrive at the likelihood normalization in eq. (???), we perform a 40th-order Gauss-Legendre integration in each $R$ and $z$ direction. The distribution function that was evaluated for these plots has the parameters $p_{DF} = \{ h_R, \sigma_R, \sigma_z,h_{\sigma_R},h_{\sigma_z}\} =\{2 \text{ kpc }, 55 \text{ km/s }, 66 \text{ km/s }, 8 \text{ kpc }, 7 \text{ kpc }\} $. We show the results for two different potentials, an isochrone potential with parameters $p_\Phi = \{v_{circ},b \}=\{230 \text{ km/s },0.9\text{ kpc } \}$ and a MW-like potential (cf. ???) with parameters $p_\Phi = \{v_{circ},R_d,z_h,f_h,\frac{\diff\ln v_c}{\diff\ln R}] \}=\{230 \text{ km/s },2.5\text{ kpc },400 \text{ pc }, 0.8,0\}$. (Caption continues on next page.)} 
\end{figure}

\addtocounter{figure}{-1}
\begin{figure} [t!]
  \caption{(Continued.) We calculate the true normalization as $M_\text{tot,true} \approx M_\text{tot}(N_\text{spatial}=20,N_\text{velocity}=56,N_\text{sigma}=7)$, which has high enough accuracy. The relative error of the normalization is then calculated as $(M_\text{tot}[N_\text{spatial},N_\text{velocity},N_\text{sigma}] -  M_\text{tot,true} ) / M_\text{tot,true} $. The dashed lines indicate the accuracy used in our analyses: it is better than $0.001\%$ for both potential types (except the smallest volume in MW potential [TO DO: Why???]). Choosing $N_\text{velocity}$ too small can have much more severe effects on the accuracy of the normalization than $N_\text{spatial}$. A relative error of $0.3\%$ can already introduce biases in the outcome of the analysis, that are, for a data set of $~10,000$ stars already as large as a few sigma. When using a fiducial qDF to set the integration range over the velocity in the analysis  (cf. \S ???) of an isochrone potential for 20,000 stars, already $N_\text{sigma}=4$ and $N_\text{velocity}=20$ behaves well: a precision of 0.05\% in the normalisation leads to an error in the log-likelihood of $\sim1$ [TO DO: Is this correct? $N \log(n+np) = N\log(n) + N\log(1+p)$ with $N\log(1+p)\sim 1$ for N=20000, p=0.00005. But why do we know that 1 is small enough? Argument is also different to Jo's argument with $N_{stars}*$precision=1, but I don't get that.???]  [TO DO: Does this higher accuracy make the biases in MW potential analyses smaller????]}
\end{figure}

%FIGURE: isoSphFlexMixCont

\begin{figure}
\plotone{~/BovyCode/analyzeFlexible/plots/isoSphFlexMixCont/isoSphFlexMixCont_violins.eps}
\caption{Caption [TO DO]} 
\end{figure}

%FIGURE: isoSphFlexMixDiff

\begin{figure}
\plotone{~/BovyCode/analyzeFlexible/plots/isoSphFlexMixDiff/isoSphFlexMixDiff_violins.eps}
\caption{Caption [TO DO], Maybe different/same x-axis??? [TO DO]} 
\end{figure}



%===============================================

\begin{thebibliography}{}
\bibitem[Binney \& McMillan(2011)]{bin11} Binney, J. J., \& McMillan, P. 2011, \mnras, 413, 1889
\bibitem[Binney(2012)]{bin12} Binney, J. J. 2012, \mnras, 426, 1324
\bibitem[Bovy et al.(2012b)]{bov12b} Bovy, J., Rix, H.-W., \& Hogg, D. W. 2012b, \apj, 751, 131
\bibitem[Bovy et al.(2012c)]{bov12c} Bovy, J., Rix, H.-W., Hogg, D. W. et al., 2012c, \apj, 755,115
\bibitem[Bovy et al.(2012d)]{bov12d} Bovy, J., Rix, H.-W., Liu, C. et al., 2012d, \apj, 753, 148
\bibitem[Bovy \& Rix(2013)]{bov13}  Bovy, J., \& Rix, H.-W. 2003, \apj, 779, 115
\bibitem[Ting et al.(2013)]{tin13} Ting, Y.-S., Rix, H.-W., Bovy, J., \& van de Ven, G. 2013, \mnras, 434, 652
\end{thebibliography}



\end{document}
